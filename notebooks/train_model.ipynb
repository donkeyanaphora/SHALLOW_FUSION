{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Details (GPT-3 Paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model Name              | nparams | nlayers | dmodel | nheads | dhead | Batch Size | Learning Rate  |\n",
    "|-------------------------|---------|---------|--------|--------|-------|------------|----------------|\n",
    "| GPT-3 Small             | 125M    | 12      | 768    | 12     | 64    | 0.5M       | 6.0 × 10−4    |\n",
    "| GPT-3 Medium            | 350M    | 24      | 1024   | 16     | 64    | 0.5M       | 3.0 × 10−4    |\n",
    "| GPT-3 Large             | 760M    | 24      | 1536   | 16     | 96    | 0.5M       | 2.5 × 10−4    |\n",
    "| GPT-3 XL                | 1.3B    | 24      | 2048   | 24     | 128   | 1M         | 2.0 × 10−4    |\n",
    "| GPT-3 2.7B              | 2.7B    | 32      | 2560   | 32     | 80    | 1M         | 1.6 × 10−4    |\n",
    "| GPT-3 6.7B              | 6.7B    | 32      | 4096   | 32     | 128   | 2M         | 1.2 × 10−4    |\n",
    "| GPT-3 13B               | 13.0B   | 40      | 5140   | 40     | 128   | 2M         | 1.0 × 10−4    |\n",
    "| GPT-3 175B or “GPT-3”    | 175.0B  | 96      | 12288  | 96     | 128   | 3.2M       | 0.6 × 10−4    |\n",
    "\n",
    "**Table 2.1:** Sizes, architectures, and learning hyper-parameters (batch size in tokens and learning rate) of the models\n",
    "which we trained. All models were trained for a total of 300 billion tokens.\n",
    "\n",
    "\n",
    "**Table 2.1** shows the sizes and architectures of our 8 models. Here nparams is the total number of trainable parameters,\n",
    "nlayers is the total number of layers, dmodel is the number of units in each bottleneck layer (we always have the\n",
    "feedforward layer four times the size of the bottleneck layer, dff = 4 ∗ dmodel), and dhead is the dimension of each\n",
    "attention head. All models use a context window of nctx = 2048 tokens. We partition the model across GPUs along\n",
    "both the depth and width dimension in order to minimize data-transfer between nodes. The precise architectural\n",
    "parameters for each model are chosen based on computational efficiency and load-balancing in the layout of models\n",
    "across GPU’s. Previous work [KMH+20 ] suggests that validation loss is not strongly sensitive to these parameters\n",
    "within a reasonably broad range.\n",
    "\n",
    "#### B Details of Model Training\n",
    "\n",
    "To train all versions of GPT-3, we use **Adam** with **β1 = 0.9**, **β2 = 0.95**, and **ε = 10⁻⁸**, clip the global norm of the gradient at **1.0**, and apply **cosine decay** for the learning rate, reducing it to **10%** of its value over **260 billion tokens** (after which training continues at 10% of the original rate). There is a **linear learning rate warmup** over the first **375 million tokens**, and the batch size is gradually increased from **32k tokens** to the full value over the first **4–12 billion tokens** of training, depending on model size. Data are sampled without replacement until an epoch boundary is reached to minimize overfitting, and all models use a **weight decay of 0.1** for regularization. During training, we always use sequences of the full **2048-token context window**, packing multiple documents into a single sequence when documents are shorter than 2048, with a special **end of text token** delimiting documents to efficiently indicate that separated contexts are unrelated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login to the hub to push checkpoints and final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "import getpass\n",
    "import os \n",
    "\n",
    "hf_token = getpass.getpass(\"Enter your Hugging Face token: \")\n",
    "os.environ[\"HF_TOKEN\"] = hf_token\n",
    "repo_name  = \"cwestnedge/gpt2_test\"\n",
    "api = HfApi(token=os.environ[\"HF_TOKEN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, glob\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from transformers import GPT2LMHeadModel, get_scheduler\n",
    "import time\n",
    "import re\n",
    "\n",
    "class PTIterableDataset(IterableDataset):\n",
    "    def __init__(self, pt_files):\n",
    "        self.pt_files = pt_files\n",
    "\n",
    "    def __iter__(self):\n",
    "        for file_path in self.pt_files:\n",
    "            data = torch.load(file_path)\n",
    "            for i in range(data[\"input_ids\"].size(0)):\n",
    "                sample = {\n",
    "                    \"input_ids\": data[\"input_ids\"][i],\n",
    "                    \"attention_mask\": data[\"attention_mask\"][i],\n",
    "                    \"files\": file_path.split('/')[-1]\n",
    "                }\n",
    "                if data.get(\"labels\") is not None:\n",
    "                    sample[\"labels\"] = data[\"labels\"][i]\n",
    "                yield sample\n",
    "\n",
    "\n",
    "# really sorry this function is necessary but i screwed up the file naming conventions\n",
    "# so this is my lame patch\n",
    "def extract_number(filename):\n",
    "    match = re.search(r'(\\d+)', filename)\n",
    "    return int(match.group(1)) if match else 0\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "loader_batch_size = 4\n",
    "train_files = sorted(glob.glob(\"../processed_batches/train/*.pt\"), key=extract_number)\n",
    "test_files = sorted(glob.glob(\"../processed_batches/test/*.pt\"), key=extract_number)\n",
    "\n",
    "train_loader = DataLoader(PTIterableDataset(train_files), batch_size=loader_batch_size, num_workers=0)\n",
    "test_loader = DataLoader(PTIterableDataset(test_files), batch_size=loader_batch_size, num_workers=0)\n",
    "\n",
    "print('-'*50 + 'TRAIN' + '-'*50)\n",
    "train = next(iter(train_loader))\n",
    "print(train)\n",
    "print(train['input_ids'].shape)\n",
    "\n",
    "print('-'*50 + 'TEST' + '-'*50)\n",
    "test = next(iter(test_loader))\n",
    "print(test)\n",
    "print(test['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Hyperparapeters (currently using ones for models on scale with gpt3-large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === METADATA ===\n",
    "gpt3_warmup_step_ratio = (375_000_000/260_000_000_000) # per the gpt3 paper treating it as a ratio which may not be the best\n",
    "n_files = 221713\n",
    "n_tokens_per_file = 16*1024 # (file_batch_sixe x max_token_len)\n",
    "total_tokens = n_files * n_tokens_per_file\n",
    "\n",
    "# === HYPERPARAMETERS ===\n",
    "# we want to have roughly .5M tokens per batch based on above figure for gpt3\n",
    "gradient_accumulation_steps = 4 # 128 \n",
    "tokens_per_batch = (n_tokens_per_file/loader_batch_size) * gradient_accumulation_steps\n",
    "print(f\"Tokens per batch (should be roughly .5M): {tokens_per_batch}\")\n",
    "\n",
    "initial_lr = 2.5e-4 # LR for should be 6e-4 to 2.5e-4 for gpt3 small-large\n",
    "n_training_steps = total_tokens / tokens_per_batch\n",
    "n_warmup_steps = int(round(n_training_steps * gpt3_warmup_step_ratio, 1))\n",
    "print(f\"N warmup steps (could be {gpt3_warmup_step_ratio*100:.2f}% of {n_training_steps} training_steps) => {n_warmup_steps} steps\")\n",
    "\n",
    "beta1, beta2 = 0.9, 0.95 # these may need to be changed to fit our training assumptions\n",
    "max_grad_norm = 1.0 # paper uses 1 i think this generalizes to our usecase\n",
    "weight_decay = .10 # i believe this still makes sense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_accumulation_steps * loader_batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions to handle weight decay and checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grouped_params(model, weight_decay, no_decay=[\"bias\", \"LayerNorm.weight\"]):\n",
    "    '''handy function for setting weight decay shoutout to hugging face book '''\n",
    "    params_with_wd, params_without_wd = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if any(nd in n for nd in no_decay):\n",
    "            params_without_wd.append(p)\n",
    "        else:\n",
    "            params_with_wd.append(p)\n",
    "    return [{'params': params_with_wd, 'weight_decay': weight_decay},\n",
    "            {'params': params_without_wd, 'weight_decay': 0.0}]\n",
    "\n",
    "def save_checkpoint_metadata(optimizer, lr_scheduler, step, losses, batch_file, scaler=None):\n",
    "    checkpoint = {\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'lr_scheduler': lr_scheduler.state_dict(),\n",
    "        'global_step': step,\n",
    "        'losses': losses,\n",
    "        'batch_file': batch_file,\n",
    "    }\n",
    "\n",
    "    # scaler is for GPU only since doing fp16 on GPU\n",
    "    if scaler is not None:\n",
    "        checkpoint['scaler'] = scaler.state_dict()\n",
    "\n",
    "    # checkpoint file locally so we can easily push to hub\n",
    "    torch.save(checkpoint, \"training_state.pt\")\n",
    "\n",
    "\n",
    "def push_model_and_state_to_hub(model, api, step, max_retries=3, retry_delay=10):\n",
    "    for attempt in range(1, max_retries+1):\n",
    "        try:\n",
    "            # FIRST push model to hub FIRST (creates repo if it doesnt exit)\n",
    "            model.push_to_hub(repo_name, commit_message=f\"Checkpoint at step {step}\")\n",
    "\n",
    "            # THEN the training state file\n",
    "            api.upload_file(\n",
    "                path_or_fileobj=\"training_state.pt\",\n",
    "                path_in_repo=\"training_state.pt\",\n",
    "                repo_id=repo_name,\n",
    "                commit_message=f\"Training state at step {step}\"\n",
    "            )\n",
    "            # then push the model\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt} failed: {e}\")\n",
    "            if attempt == max_retries:\n",
    "                print(\"Max attempts reached. Exiting.\")\n",
    "                raise e\n",
    "            time.sleep(retry_delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize model, optimizer, and scheduler for training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.compile(GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\"))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "model_params_with_decay = get_grouped_params(model, weight_decay=weight_decay)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model_params_with_decay, # default would be model.parameters()\n",
    "    lr=initial_lr,\n",
    "    )\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"cosine\", \n",
    "    optimizer=optimizer, \n",
    "    num_warmup_steps=n_warmup_steps, \n",
    "    num_training_steps=n_training_steps\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model (GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_history = []\n",
    "# global_step = 0\n",
    "# scaler = torch.amp.GradScaler(\"cuda\") # for floating point 16 (GPU only)\n",
    "\n",
    "# for step, batch in enumerate(train_loader):\n",
    "#     input_ids = batch['input_ids'].to(device)\n",
    "#     attention_mask = batch['attention_mask'].to(device)\n",
    "#     batch_file_names = set(batch['files']) # helps us keep track of where training left off at\n",
    "\n",
    "#     # for GPU only \n",
    "#     with torch.autocast(device_type=\"cuda\"):\n",
    "#         # labels and input are same since GPT2LMHeadModel will perform shift internally\n",
    "#         # if computing loss externally you will want to shift labels then pass to loss_fn\n",
    "#         outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "#         loss = outputs.loss\n",
    "\n",
    "#     loss = loss / gradient_accumulation_steps\n",
    "#     scaler.scale(loss).backward()\n",
    "\n",
    "#     # optimizer step...\n",
    "#     if (step + 1) % gradient_accumulation_steps == 0:\n",
    "#         # unscale, clip, step, update scaler & scheduler\n",
    "#         scaler.unscale_(optimizer)\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "#         scaler.step(optimizer)\n",
    "#         scaler.update()\n",
    "\n",
    "#         lr_scheduler.step()\n",
    "#         optimizer.zero_grad()\n",
    "#         global_step += 1\n",
    "\n",
    "#         loss_to_log = loss.item() * gradient_accumulation_steps\n",
    "#         loss_history.append(loss_to_log)\n",
    "#         print(f\"Global step {global_step}, loss: {loss_to_log:.4f}\")\n",
    "\n",
    "#         if global_step % 100 == 0:\n",
    "#             commit_msg = f\"Checkpoint at step {global_step}\"\n",
    "#             save_checkpoint_metadata(\n",
    "#                 optimizer=optimizer,\n",
    "#                 lr_scheduler=lr_scheduler,\n",
    "#                 scaler=scaler,\n",
    "#                 step=global_step,\n",
    "#                 losses=loss_history,\n",
    "#                 batch_file=batch_file_names\n",
    "#             )\n",
    "\n",
    "#             print('Pushing to hub...')\n",
    "#             push_model_and_state_to_hub(\n",
    "#                 model=model,\n",
    "#                 api=api,\n",
    "#                 step=global_step,\n",
    "#                 max_retries=3,\n",
    "#                 retry_delay=10\n",
    "#             )\n",
    "\n",
    "# if (step + 1) % gradient_accumulation_steps != 0:\n",
    "#     # perform a final optimizer step to flush any remaining gradients\n",
    "#     scaler.unscale_(optimizer)\n",
    "#     torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "#     scaler.step(optimizer)\n",
    "#     scaler.update()\n",
    "\n",
    "#     lr_scheduler.step()\n",
    "#     optimizer.zero_grad()\n",
    "#     global_step += 1\n",
    "\n",
    "#     loss_to_log = loss.item() * gradient_accumulation_steps\n",
    "#     loss_history.append(loss_to_log)\n",
    "\n",
    "#     print(f\"Performed final optimizer step to flush remaining gradients at global step {global_step}\")\n",
    "\n",
    "# # final commit\n",
    "# print(f'FIRST PASS COMPLETE AT STEP {step}')\n",
    "# save_checkpoint_metadata(\n",
    "#     optimizer=optimizer,\n",
    "#     lr_scheduler=lr_scheduler,\n",
    "#     scaler=scaler,\n",
    "#     step=global_step,\n",
    "#     losses=loss_history,\n",
    "#     batch_file=batch_file_names\n",
    "# )\n",
    "# push_model_and_state_to_hub(\n",
    "#     model=model,\n",
    "#     api=api,\n",
    "#     step=global_step,\n",
    "#     max_retries=3,\n",
    "#     retry_delay=10\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model (CPU for testing only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = []\n",
    "global_step = 0\n",
    "save_steps = 1\n",
    "for step, batch in enumerate(train_loader):\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    batch_file_names = set(batch['files'])  # helps us keep track of where training left off at\n",
    "\n",
    "    # forward pass (no autocast for CPU)\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "    loss = outputs.loss\n",
    "\n",
    "    loss = loss / gradient_accumulation_steps\n",
    "    loss.backward()  # Standard backward pass without scaling\n",
    "\n",
    "    if (step + 1) % gradient_accumulation_steps == 0:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        lr_scheduler.step()\n",
    "        global_step += 1\n",
    "\n",
    "        loss_to_log = loss.item() * gradient_accumulation_steps\n",
    "        loss_history.append(loss_to_log)\n",
    "        print(f\"Global step {global_step}, loss: {loss_to_log:.4f}\")\n",
    "\n",
    "        if global_step % save_steps == 0:\n",
    "            commit_msg = f\"Checkpoint at step {global_step}\"\n",
    "            save_checkpoint_metadata(\n",
    "                optimizer=optimizer,\n",
    "                lr_scheduler=lr_scheduler,\n",
    "                scaler=None,  # No scaler for CPU training\n",
    "                step=global_step,\n",
    "                losses=loss_history,\n",
    "                batch_file=batch_file_names\n",
    "            )\n",
    "\n",
    "            print('Pushing to hub...')\n",
    "            push_model_and_state_to_hub(\n",
    "                model=model,\n",
    "                api=api,\n",
    "                step=global_step,\n",
    "                max_retries=3,\n",
    "                retry_delay=10\n",
    "            )\n",
    "    \n",
    "if (step + 1) % gradient_accumulation_steps != 0:\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    lr_scheduler.step()\n",
    "    global_step += 1\n",
    "\n",
    "    loss_to_log = loss.item() * gradient_accumulation_steps\n",
    "    loss_history.append(loss_to_log)\n",
    "    print(f\"Performed final optimizer step to flush remaining gradients at global step {global_step}\")\n",
    "\n",
    "print(f'FIRST PASS COMPLETE AT STEP {step}')\n",
    "save_checkpoint_metadata(\n",
    "    optimizer=optimizer,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    scaler=None,  # no scaler for CPU training\n",
    "    step=global_step,\n",
    "    losses=loss_history,\n",
    "    batch_file=batch_file_names\n",
    ")\n",
    "push_model_and_state_to_hub(\n",
    "    model=model,\n",
    "    api=api,\n",
    "    step=global_step,\n",
    "    max_retries=3,\n",
    "    retry_delay=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import hf_hub_download, HfApi\n",
    "# import getpass\n",
    "# import os \n",
    "\n",
    "# hf_token = getpass.getpass(\"Enter your Hugging Face token: \")\n",
    "# os.environ[\"HF_TOKEN\"] = hf_token\n",
    "# repo_name  = \"cwestnedge/gpt2_test\"\n",
    "# api = HfApi(token=os.environ[\"HF_TOKEN\"])\n",
    "\n",
    "# training_state_path = hf_hub_download(\n",
    "#     repo_id=repo_name, \n",
    "#     filename=\"training_state.pt\",\n",
    "#     token=hf_token\n",
    "# )\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# checkpoint = torch.load(training_state_path, map_location=torch.device(device))\n",
    "\n",
    "# # pull relevant stuff\n",
    "# optimizer_state = checkpoint['optimizer']\n",
    "# lr_state = checkpoint['lr_scheduler']\n",
    "# global_step = checkpoint['global_step']\n",
    "# loss = checkpoint['losses']\n",
    "# last_batch = checkpoint['batch_file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import GPT2LMHeadModel\n",
    "# import torch\n",
    "\n",
    "# def get_grouped_params(model, weight_decay, no_decay=[\"bias\", \"LayerNorm.weight\"]):\n",
    "#     '''handy function for setting weight decay shoutout to hugging face book '''\n",
    "#     params_with_wd, params_without_wd = [], []\n",
    "#     for n, p in model.named_parameters():\n",
    "#         if any(nd in n for nd in no_decay):\n",
    "#             params_without_wd.append(p)\n",
    "#         else:\n",
    "#             params_with_wd.append(p)\n",
    "#     return [{'params': params_with_wd, 'weight_decay': weight_decay},\n",
    "#             {'params': params_without_wd, 'weight_decay': 0.0}]\n",
    "\n",
    "\n",
    "# model_t = GPT2LMHeadModel.from_pretrained(repo_name, token=hf_token)\n",
    "# model_t.to(device);\n",
    "# grouped_params = get_grouped_params(model_t, weight_decay=.10)\n",
    "\n",
    "# optimizer = torch.optim.Adam(\n",
    "#     grouped_params, \n",
    "#     lr=lr_state['_last_lr'][0]\n",
    "#     )\n",
    "# optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "# n_warmup_steps = 100  # example value adjust accordingly\n",
    "# n_training_steps = 1000  # example value adjust accordingly\n",
    "# lr_scheduler = get_scheduler(\n",
    "#     name=\"cosine\", \n",
    "#     optimizer=optimizer, \n",
    "#     num_warmup_steps=n_warmup_steps, # original values\n",
    "#     num_training_steps=n_training_steps # original value\n",
    "#     )\n",
    "# lr_scheduler.load_state_dict(lr_state)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
