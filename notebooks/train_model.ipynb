{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-3 Paper Info\n",
    "\n",
    "### Model Architectures and Hyper-Parameters\n",
    "\n",
    "| Model Name              | nparams | nlayers | dmodel | nheads | dhead | Batch Size | Learning Rate  |\n",
    "|-------------------------|---------|---------|--------|--------|-------|------------|----------------|\n",
    "| GPT-3 Small             | 125M    | 12      | 768    | 12     | 64    | 0.5M       | 6.0 × 10−4    |\n",
    "| GPT-3 Medium            | 350M    | 24      | 1024   | 16     | 64    | 0.5M       | 3.0 × 10−4    |\n",
    "| GPT-3 Large             | 760M    | 24      | 1536   | 16     | 96    | 0.5M       | 2.5 × 10−4    |\n",
    "| GPT-3 XL                | 1.3B    | 24      | 2048   | 24     | 128   | 1M         | 2.0 × 10−4    |\n",
    "| GPT-3 2.7B              | 2.7B    | 32      | 2560   | 32     | 80    | 1M         | 1.6 × 10−4    |\n",
    "| GPT-3 6.7B              | 6.7B    | 32      | 4096   | 32     | 128   | 2M         | 1.2 × 10−4    |\n",
    "| GPT-3 13B               | 13.0B   | 40      | 5140   | 40     | 128   | 2M         | 1.0 × 10−4    |\n",
    "| GPT-3 175B or “GPT-3”    | 175.0B  | 96      | 12288  | 96     | 128   | 3.2M       | 0.6 × 10−4    |\n",
    "\n",
    "**Table 2.1:** Sizes, architectures, and learning hyper-parameters (batch size in tokens and learning rate) of the models\n",
    "which we trained. All models were trained for a total of 300 billion tokens.\n",
    "\n",
    "\n",
    "**Table 2.1** shows the sizes and architectures of our 8 models. Here nparams is the total number of trainable parameters,\n",
    "nlayers is the total number of layers, dmodel is the number of units in each bottleneck layer (we always have the\n",
    "feedforward layer four times the size of the bottleneck layer, dff = 4 ∗ dmodel), and dhead is the dimension of each\n",
    "attention head. All models use a context window of nctx = 2048 tokens. We partition the model across GPUs along\n",
    "both the depth and width dimension in order to minimize data-transfer between nodes. The precise architectural\n",
    "parameters for each model are chosen based on computational efficiency and load-balancing in the layout of models\n",
    "across GPU’s. Previous work [KMH+20 ] suggests that validation loss is not strongly sensitive to these parameters\n",
    "within a reasonably broad range.\n",
    "\n",
    "#### B Details of Model Training\n",
    "\n",
    "To train all versions of GPT-3, we use **Adam** with **β1 = 0.9**, **β2 = 0.95**, and **ε = 10⁻⁸**, clip the global norm of the gradient at **1.0**, and apply **cosine decay** for the learning rate, reducing it to **10%** of its value over **260 billion tokens** (after which training continues at 10% of the original rate). There is a **linear learning rate warmup** over the first **375 million tokens**, and the batch size is gradually increased from **32k tokens** to the full value over the first **4–12 billion tokens** of training, depending on model size. Data are sampled without replacement until an epoch boundary is reached to minimize overfitting, and all models use a **weight decay of 0.1** for regularization. During training, we always use sequences of the full **2048-token context window**, packing multiple documents into a single sequence when documents are shorter than 2048, with a special **end of text token** delimiting documents to efficiently indicate that separated contexts are unrelated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass, os, torch, glob, re, time\n",
    "\n",
    "from transformers import GPT2LMHeadModel, get_scheduler\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from huggingface_hub import hf_hub_download, HfApi, create_repo\n",
    "from huggingface_hub.utils import HfHubHTTPError\n",
    "\n",
    "class PTIterableDataset(IterableDataset):\n",
    "    def __init__(self, pt_files):\n",
    "        self.pt_files = pt_files\n",
    "\n",
    "    def __iter__(self):\n",
    "        for file_path in self.pt_files:\n",
    "            data = torch.load(file_path)\n",
    "            for i in range(data[\"input_ids\"].size(0)):\n",
    "                sample = {\n",
    "                    \"input_ids\": data[\"input_ids\"][i],\n",
    "                    \"attention_mask\": data[\"attention_mask\"][i],\n",
    "                    \"files\": file_path.split('/')[-1]\n",
    "                }\n",
    "                if data.get(\"labels\") is not None:\n",
    "                    sample[\"labels\"] = data[\"labels\"][i]\n",
    "                yield sample\n",
    "\n",
    "def load_checkpoint(repo_name, token, device, file_name=\"training_state.pt\"):\n",
    "    repo_name = repo_name\n",
    "\n",
    "    training_state_path = hf_hub_download(\n",
    "        repo_id=repo_name, \n",
    "        filename=file_name,\n",
    "        token=token\n",
    "    )\n",
    "    checkpoint = torch.load(training_state_path, map_location=torch.device(device))\n",
    "    return checkpoint\n",
    "\n",
    "def get_grouped_params(model, weight_decay, no_decay=[\"bias\", \"LayerNorm.weight\"]):\n",
    "    '''handy function for setting weight decay shoutout to hugging face book '''\n",
    "    params_with_wd, params_without_wd = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if any(nd in n for nd in no_decay):\n",
    "            params_without_wd.append(p)\n",
    "        else:\n",
    "            params_with_wd.append(p)\n",
    "    return [{'params': params_with_wd, 'weight_decay': weight_decay},\n",
    "            {'params': params_without_wd, 'weight_decay': 0.0}]\n",
    "\n",
    "\n",
    "def load_base_model(model_name, device):\n",
    "    model = torch.compile(GPT2LMHeadModel.from_pretrained(model_name))\n",
    "    return model.to(device)\n",
    "\n",
    "def initialize_optimizer(model_params, base_lr):\n",
    "    optimizer = torch.optim.Adam(\n",
    "        params=model_params,\n",
    "        lr=base_lr\n",
    "    )\n",
    "    return optimizer\n",
    "\n",
    "def initialize_scheduler(optimizer, n_warmup_steps, n_training_steps):\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=\"cosine\", \n",
    "        optimizer=optimizer, \n",
    "        num_warmup_steps=n_warmup_steps, \n",
    "        num_training_steps=n_training_steps\n",
    "    )\n",
    "    return lr_scheduler\n",
    "    \n",
    "def initialize_scaler(device):\n",
    "    return torch.amp.GradScaler(\"cuda\") if device == 'cuda' else None\n",
    "\n",
    "def extract_file_numbers(filename):\n",
    "    match = re.search(r'(\\d+)', filename)\n",
    "    return int(match.group(1)) if match else 0\n",
    "\n",
    "def save_checkpoint(model, optimizer, lr_scheduler, global_step, loss_history, last_file, scaler=None):\n",
    "    checkpoint = {\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'lr_scheduler': lr_scheduler.state_dict(),\n",
    "        'global_step': global_step,\n",
    "        'losses': loss_history,\n",
    "        'batch_file': last_file,\n",
    "    }\n",
    "\n",
    "    # scaler is for GPU only since doing fp16 on GPU\n",
    "    if scaler is not None:\n",
    "        checkpoint['scaler'] = scaler.state_dict()\n",
    "\n",
    "    # checkpoint file locally so we can easily push to hub\n",
    "    torch.save(checkpoint, \"training_state.pt\")\n",
    "\n",
    "\n",
    "def create_repo_if_not_exists(repo_name, token):\n",
    "    api = HfApi(token=token)\n",
    "    try:\n",
    "        api.repo_info(repo_id=repo_name)\n",
    "        print(f\"Repository '{repo_name}' already exists.\")\n",
    "    except HfHubHTTPError as e:\n",
    "        if e.response.status_code == 404:\n",
    "            print(f\"Repository '{repo_name}' not found. Creating repository...\")\n",
    "            create_repo(repo_id=repo_name, token=token)\n",
    "            print(f\"Repository '{repo_name}' created successfully.\")\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "\n",
    "def push_to_hub(repo_name, token, step, max_retries=3, retry_delay=10):\n",
    "    api = HfApi(token=token)\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            # Upload the training state file.\n",
    "            api.upload_file(\n",
    "                path_or_fileobj=\"training_state.pt\",\n",
    "                path_in_repo=\"training_state.pt\",\n",
    "                repo_id=repo_name,\n",
    "                commit_message=f\"Training state at step {step}\"\n",
    "            )\n",
    "            # Here you can add code to push the model.\n",
    "            print(\"Training state (and model if implemented) pushed successfully.\")\n",
    "            break  # Exit the loop if the upload succeeds.\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt} failed: {e}\")\n",
    "            if attempt == max_retries:\n",
    "                print(\"Max attempts reached. Exiting.\")\n",
    "                raise e\n",
    "            time.sleep(retry_delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective size with grad accumulation: 8\n",
      "Tokens per batch (paper has roughly .5M): 8192.0\n",
      "Total Training steps 443426.0\n",
      "N warmup steps (could be 3.00% of 443426.0 training_steps) => 13302 steps\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    device: str = 'cpu'\n",
    "    from_checkpoint: bool = False\n",
    "    data_loader_batch_size = 4\n",
    "    warm_up_ratio: float = 0.03\n",
    "\n",
    "    n_files: int = 221713\n",
    "    n_tokens_per_file: int = 16*1024 # (file_batch_size x max_token_len)\n",
    "    total_tokens: int = n_files * n_tokens_per_file\n",
    "    gradient_accumulation_steps: int = 2\n",
    "    tokens_per_batch: int = (n_tokens_per_file/data_loader_batch_size) * gradient_accumulation_steps\n",
    "    print(f\"Effective size with grad accumulation: {data_loader_batch_size*gradient_accumulation_steps}\")\n",
    "    print(f\"Tokens per batch (paper has roughly .5M): {tokens_per_batch}\")\n",
    "\n",
    "    base_lr: float = 1e-4 # LR for should be 6e-4 to 2.5e-4 for gpt3 small-large\n",
    "    n_training_steps: float = total_tokens / tokens_per_batch\n",
    "    n_warmup_steps: int = int(round(n_training_steps * warm_up_ratio, 1))\n",
    "    print(f\"Total Training steps {n_training_steps}\")\n",
    "    print(f\"N warmup steps (could be {warm_up_ratio*100:.2f}% of {n_training_steps} training_steps) => {n_warmup_steps} steps\")\n",
    "\n",
    "    # beta1, beta2 = 0.9, 0.95 # these may need to be changed to fit our training assumptions\n",
    "    max_grad_norm = 1.0 # paper uses 1\n",
    "    weight_decay = .10 # i believe this still makes sense\n",
    "\n",
    "    checkpoint_repo: str = None\n",
    "    save_file_name: str = \"training_state.pt\"\n",
    "    hf_token: str = None\n",
    "    start_file: str = None\n",
    "    save_steps = 10\n",
    "\n",
    "config = Config()\n",
    "config.from_checkpoint = True\n",
    "config.checkpoint_repo = \"cwestnedge/gpt2-test\"\n",
    "config.base_model = \"openai-community/gpt2\"\n",
    "config.hf_token = getpass.getpass(\"Enter your Hugging Face token: \")\n",
    "config.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Last processed file batch_0007.pt. Resuming run from ../processed_batches/train/batch_0008.pt\n",
      "99.996% remaining...\n",
      "{'input_ids': tensor([[ 9021,   547,  6157,  ...,  2482,   625,  7446],\n",
      "        [22236,    12,  3106,  ...,  2310,    25,  1065],\n",
      "        [   12,  1507,    11,  ...,   689,   286, 32450],\n",
      "        [  259,   351, 11354,  ...,   329,   262,  1692]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'files': ['batch_0008.pt', 'batch_0008.pt', 'batch_0008.pt', 'batch_0008.pt']}\n",
      "\n",
      "Repository 'cwestnedge/gpt2-test' already exists.\n"
     ]
    }
   ],
   "source": [
    "model = load_base_model(model_name=config.base_model, device=config.device)\n",
    "model_grouped_params = get_grouped_params(model, weight_decay=config.weight_decay)\n",
    "optimizer = initialize_optimizer(model_grouped_params, base_lr=config.base_lr)\n",
    "lr_scheduler = initialize_scheduler(\n",
    "    n_warmup_steps=config.n_warmup_steps, \n",
    "    n_training_steps=config.n_training_steps, \n",
    "    optimizer=optimizer\n",
    ")\n",
    "scaler = initialize_scaler(config.device)\n",
    "\n",
    "if config.from_checkpoint: \n",
    "    checkpoint = load_checkpoint(\n",
    "        repo_name=config.checkpoint_repo,\n",
    "        token=config.hf_token,\n",
    "        device=config.device, \n",
    "        file_name=config.save_file_name\n",
    "    )\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model']) # we want to log model state dict eventually model.load_state_dict(model.state_dict())\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "    if scaler:\n",
    "        scaler.load_state_dict(checkpoint['scaler'])\n",
    "    \n",
    "    global_step = checkpoint['global_step']\n",
    "    loss_history = checkpoint['losses']\n",
    "    last_file = checkpoint['batch_file']\n",
    "    last_file = ''.join(last_file)\n",
    "\n",
    "    train_files_full = sorted(glob.glob(\"../processed_batches/train/*.pt\"), key=extract_file_numbers)\n",
    "    start_file_path = f'../processed_batches/train/{last_file}'\n",
    "    start_idx = train_files_full.index(start_file_path)\n",
    "    train_files_ = train_files_full[start_idx+1:]\n",
    "    print()\n",
    "    print(f'Last processed file {last_file}. Resuming run from {train_files_[0]}')\n",
    "    print(f\"{(len(train_files_)/len(train_files_full))*100:0.3f}% remaining...\")\n",
    "\n",
    "else:\n",
    "    global_step, loss_history= 0, []\n",
    "    train_files_ = sorted(glob.glob(\"../processed_batches/train/*.pt\"), key=extract_file_numbers)\n",
    "    print()\n",
    "    print(f'training run from {train_files_[0]}')\n",
    "\n",
    "train_loader = DataLoader(PTIterableDataset(train_files_), batch_size=config.data_loader_batch_size, num_workers=0, drop_last=True)\n",
    "print(next(iter(train_loader)))\n",
    "print()\n",
    "create_repo_if_not_exists(config.checkpoint_repo, config.hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU Training Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global step 10, loss: 3.3868\n",
      "saved checkpoint\n",
      "Repository 'cwestnedge/gpt2-test' already exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training_state.pt: 100%|██████████| 1.49G/1.49G [00:52<00:00, 28.6MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training state (and model if implemented) pushed successfully.\n",
      "hub push completed\n",
      "Global step 11, loss: 3.2753\n",
      "saved checkpoint\n",
      "Repository 'cwestnedge/gpt2-test' already exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training_state.pt: 100%|██████████| 1.49G/1.49G [00:58<00:00, 25.3MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training state (and model if implemented) pushed successfully.\n",
      "hub push completed\n",
      "Global step 12, loss: 3.2361\n",
      "saved checkpoint\n",
      "Repository 'cwestnedge/gpt2-test' already exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training_state.pt: 100%|██████████| 1.49G/1.49G [00:59<00:00, 25.2MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training state (and model if implemented) pushed successfully.\n",
      "hub push completed\n"
     ]
    }
   ],
   "source": [
    "running_loss = 0 \n",
    "counter = 0 \n",
    "for step, batch in enumerate(train_loader): \n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask = batch['attention_mask']\n",
    "    current_file = set(batch['files'])\n",
    "\n",
    "    # forward pass (no autocast for CPU)\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "    raw_loss = outputs.loss\n",
    "\n",
    "    running_loss+=raw_loss.item()\n",
    "    loss = raw_loss/config.gradient_accumulation_steps\n",
    "    loss.backward()\n",
    "\n",
    "    if (step+1)%config.gradient_accumulation_steps==0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        lr_scheduler.step()\n",
    "        global_step+=1\n",
    "\n",
    "        effective_loss = running_loss/config.gradient_accumulation_steps\n",
    "        loss_history.append(effective_loss)\n",
    "        print(f\"Global step {global_step}, loss: {effective_loss:.4f}\")\n",
    "        running_loss = 0 \n",
    "\n",
    "        if global_step%config.save_steps==0:\n",
    "            save_checkpoint(\n",
    "                model=model, \n",
    "                optimizer=optimizer, \n",
    "                lr_scheduler=lr_scheduler, \n",
    "                global_step=global_step, \n",
    "                loss_history=loss_history,\n",
    "                last_file=current_file,\n",
    "            )\n",
    "            \n",
    "            print('saved checkpoint')\n",
    "            push_to_hub(\n",
    "                repo_name=config.checkpoint_repo,\n",
    "                token=config.hf_token,\n",
    "                step=global_step,\n",
    "                max_retries=3,\n",
    "                retry_delay=10\n",
    "            )\n",
    "            print('hub push completed')\n",
    "            counter+=1\n",
    "            if counter > 2: break\n",
    "\n",
    "print('final model push...')\n",
    "model.push_to_hub(config.checkpoint_repo, commit_message=\"Final trained model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Training Loop (FP16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_loss = 0 \n",
    "counter = 0 \n",
    "for step, batch in enumerate(train_loader): \n",
    "    input_ids = batch['input_ids'].to(config.device)\n",
    "    attention_mask = batch['attention_mask'].to(config.device)\n",
    "    current_file = batch['files'][0]  # single filename for consistency\n",
    "\n",
    "    with torch.autocast(device_type=\"cuda\"):\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "        raw_loss = outputs.loss\n",
    "\n",
    "    running_loss += raw_loss.item()\n",
    "    loss = raw_loss / config.gradient_accumulation_steps\n",
    "\n",
    "    if scaler:\n",
    "        scaler.scale(loss).backward()\n",
    "    else:\n",
    "        loss.backward()\n",
    "\n",
    "    if (step + 1) % config.gradient_accumulation_steps == 0:\n",
    "        # unscale, clip, step, update scaler & scheduler\n",
    "        if scaler:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        lr_scheduler.step()\n",
    "        global_step += 1\n",
    "\n",
    "        effective_loss = running_loss / config.gradient_accumulation_steps\n",
    "        loss_history.append(effective_loss)\n",
    "        print(f\"Global step {global_step}, loss: {effective_loss:.4f}\")\n",
    "        running_loss = 0 \n",
    "\n",
    "        if global_step % config.save_steps == 0:\n",
    "            save_checkpoint(\n",
    "                model=model, \n",
    "                optimizer=optimizer, \n",
    "                lr_scheduler=lr_scheduler, \n",
    "                global_step=global_step, \n",
    "                loss_history=loss_history,\n",
    "                last_file=current_file,\n",
    "                scaler=scaler\n",
    "            )\n",
    "            print('saved checkpoint')\n",
    "            push_to_hub(\n",
    "                repo_name=config.checkpoint_repo,\n",
    "                token=config.hf_token,\n",
    "                step=global_step\n",
    "            )\n",
    "            print('hub push completed')\n",
    "            counter+=1\n",
    "        if counter > 2: \n",
    "            break\n",
    "\n",
    "print('final model push...')\n",
    "model.push_to_hub(config.checkpoint_repo, commit_message=\"Final trained model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
