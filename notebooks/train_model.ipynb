{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-3 Paper Info\n",
    "\n",
    "### Model Architectures and Hyper-Parameters\n",
    "\n",
    "| Model Name              | nparams | nlayers | dmodel | nheads | dhead | Batch Size | Learning Rate  |\n",
    "|-------------------------|---------|---------|--------|--------|-------|------------|----------------|\n",
    "| GPT-3 Small             | 125M    | 12      | 768    | 12     | 64    | 0.5M       | 6.0 × 10−4    |\n",
    "| GPT-3 Medium            | 350M    | 24      | 1024   | 16     | 64    | 0.5M       | 3.0 × 10−4    |\n",
    "| GPT-3 Large             | 760M    | 24      | 1536   | 16     | 96    | 0.5M       | 2.5 × 10−4    |\n",
    "| GPT-3 XL                | 1.3B    | 24      | 2048   | 24     | 128   | 1M         | 2.0 × 10−4    |\n",
    "| GPT-3 2.7B              | 2.7B    | 32      | 2560   | 32     | 80    | 1M         | 1.6 × 10−4    |\n",
    "| GPT-3 6.7B              | 6.7B    | 32      | 4096   | 32     | 128   | 2M         | 1.2 × 10−4    |\n",
    "| GPT-3 13B               | 13.0B   | 40      | 5140   | 40     | 128   | 2M         | 1.0 × 10−4    |\n",
    "| GPT-3 175B or “GPT-3”    | 175.0B  | 96      | 12288  | 96     | 128   | 3.2M       | 0.6 × 10−4    |\n",
    "\n",
    "**Table 2.1:** Sizes, architectures, and learning hyper-parameters (batch size in tokens and learning rate) of the models\n",
    "which we trained. All models were trained for a total of 300 billion tokens.\n",
    "\n",
    "\n",
    "**Table 2.1** shows the sizes and architectures of our 8 models. Here nparams is the total number of trainable parameters,\n",
    "nlayers is the total number of layers, dmodel is the number of units in each bottleneck layer (we always have the\n",
    "feedforward layer four times the size of the bottleneck layer, dff = 4 ∗ dmodel), and dhead is the dimension of each\n",
    "attention head. All models use a context window of nctx = 2048 tokens. We partition the model across GPUs along\n",
    "both the depth and width dimension in order to minimize data-transfer between nodes. The precise architectural\n",
    "parameters for each model are chosen based on computational efficiency and load-balancing in the layout of models\n",
    "across GPU’s. Previous work [KMH+20 ] suggests that validation loss is not strongly sensitive to these parameters\n",
    "within a reasonably broad range.\n",
    "\n",
    "#### B Details of Model Training\n",
    "\n",
    "To train all versions of GPT-3, we use **Adam** with **β1 = 0.9**, **β2 = 0.95**, and **ε = 10⁻⁸**, clip the global norm of the gradient at **1.0**, and apply **cosine decay** for the learning rate, reducing it to **10%** of its value over **260 billion tokens** (after which training continues at 10% of the original rate). There is a **linear learning rate warmup** over the first **375 million tokens**, and the batch size is gradually increased from **32k tokens** to the full value over the first **4–12 billion tokens** of training, depending on model size. Data are sampled without replacement until an epoch boundary is reached to minimize overfitting, and all models use a **weight decay of 0.1** for regularization. During training, we always use sequences of the full **2048-token context window**, packing multiple documents into a single sequence when documents are shorter than 2048, with a special **end of text token** delimiting documents to efficiently indicate that separated contexts are unrelated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/collinswestnedge/Desktop/programming/git_hub/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import getpass, os, torch, glob, re, time\n",
    "\n",
    "from transformers import GPT2LMHeadModel, get_scheduler\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from huggingface_hub import hf_hub_download, HfApi, create_repo, Repository\n",
    "from huggingface_hub.utils import HfHubHTTPError\n",
    "\n",
    "class PTIterableDataset(IterableDataset):\n",
    "    def __init__(self, pt_files):\n",
    "        self.pt_files = pt_files\n",
    "\n",
    "    def __iter__(self):\n",
    "        for file_path in self.pt_files:\n",
    "            data = torch.load(file_path)\n",
    "            for i in range(data[\"input_ids\"].size(0)):\n",
    "                sample = {\n",
    "                    \"input_ids\": data[\"input_ids\"][i],\n",
    "                    \"attention_mask\": data[\"attention_mask\"][i],\n",
    "                    \"files\": file_path.split('/')[-1]\n",
    "                }\n",
    "                if data.get(\"labels\") is not None:\n",
    "                    sample[\"labels\"] = data[\"labels\"][i]\n",
    "                yield sample\n",
    "\n",
    "# class PTIterableDataset(IterableDataset):\n",
    "#     def __init__(self, pt_files, start=0, end=None):\n",
    "#         self.pt_files = pt_files\n",
    "#         self.start = start\n",
    "#         # If no end index is provided, use infinity so that all samples after start are yielded\n",
    "#         self.end = end if end is not None else float(\"inf\")\n",
    "\n",
    "#     def __iter__(self):\n",
    "#         global_index = 0  # Global index across all files\n",
    "        \n",
    "#         for file_path in self.pt_files:\n",
    "#             data = torch.load(file_path)\n",
    "#             num_samples = data[\"input_ids\"].size(0)\n",
    "#             for i in range(num_samples):\n",
    "#                 # Skip samples until we reach the specified start index.\n",
    "#                 if global_index < self.start:\n",
    "#                     global_index += 1\n",
    "#                     continue\n",
    "\n",
    "#                 # Stop yielding once the global index reaches the specified end index.\n",
    "#                 if global_index >= self.end:\n",
    "#                     return\n",
    "\n",
    "#                 sample = {\n",
    "#                     \"input_ids\": data[\"input_ids\"][i],\n",
    "#                     \"attention_mask\": data[\"attention_mask\"][i],\n",
    "#                     \"files\": file_path.split('/')[-1]\n",
    "#                 }\n",
    "#                 if data.get(\"labels\") is not None:\n",
    "#                     sample[\"labels\"] = data[\"labels\"][i]\n",
    "#                 yield sample\n",
    "\n",
    "#                 global_index += 1\n",
    "\n",
    "def load_checkpoint(repo_name, token, device, file_name=\"training_state.pt\"):\n",
    "    repo_name = repo_name\n",
    "\n",
    "    training_state_path = hf_hub_download(\n",
    "        repo_id=repo_name, \n",
    "        filename=file_name,\n",
    "        token=token\n",
    "    )\n",
    "    checkpoint = torch.load(training_state_path, map_location=torch.device(device))\n",
    "    return checkpoint\n",
    "\n",
    "def get_grouped_params(model, weight_decay, no_decay=[\"bias\", \"LayerNorm.weight\"]):\n",
    "    '''handy function for setting weight decay shoutout to hugging face book '''\n",
    "    params_with_wd, params_without_wd = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if any(nd in n for nd in no_decay):\n",
    "            params_without_wd.append(p)\n",
    "        else:\n",
    "            params_with_wd.append(p)\n",
    "    return [{'params': params_with_wd, 'weight_decay': weight_decay},\n",
    "            {'params': params_without_wd, 'weight_decay': 0.0}]\n",
    "\n",
    "\n",
    "def load_base_model(model_name, device):\n",
    "    model = torch.compile(GPT2LMHeadModel.from_pretrained(model_name))\n",
    "    return model.to(device)\n",
    "\n",
    "def initialize_optimizer(model_params, base_lr):\n",
    "    optimizer = torch.optim.Adam(\n",
    "        params=model_params,\n",
    "        lr=base_lr\n",
    "    )\n",
    "    return optimizer\n",
    "\n",
    "def initialize_scheduler(optimizer, n_warmup_steps, n_training_steps):\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=\"cosine\", \n",
    "        optimizer=optimizer, \n",
    "        num_warmup_steps=n_warmup_steps, \n",
    "        num_training_steps=n_training_steps\n",
    "    )\n",
    "    return lr_scheduler\n",
    "    \n",
    "def initialize_scaler(device):\n",
    "    return torch.amp.GradScaler(\"cuda\") if device == 'cuda' else None\n",
    "\n",
    "def extract_file_numbers(filename):\n",
    "    match = re.search(r'(\\d+)', filename)\n",
    "    return int(match.group(1)) if match else 0\n",
    "\n",
    "def save_checkpoint(model, optimizer, lr_scheduler, global_step, loss_history, last_file, scaler=None):\n",
    "    checkpoint = {\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'lr_scheduler': lr_scheduler.state_dict(),\n",
    "        'global_step': global_step,\n",
    "        'losses': loss_history,\n",
    "        'batch_file': last_file,\n",
    "    }\n",
    "\n",
    "    # scaler is for GPU only since doing fp16 on GPU\n",
    "    if scaler is not None:\n",
    "        checkpoint['scaler'] = scaler.state_dict()\n",
    "\n",
    "    # checkpoint file locally so we can easily push to hub\n",
    "    torch.save(checkpoint, \"training_state.pt\")\n",
    "\n",
    "\n",
    "def create_repo_if_not_exists(repo_name, token):\n",
    "    api = HfApi(token=token)\n",
    "    try:\n",
    "        api.repo_info(repo_id=repo_name)\n",
    "        print(f\"Repository '{repo_name}' already exists.\")\n",
    "    except HfHubHTTPError as e:\n",
    "        if e.response.status_code == 404:\n",
    "            print(f\"Repository '{repo_name}' not found. Creating repository...\")\n",
    "            create_repo(repo_id=repo_name, token=token)\n",
    "            print(f\"Repository '{repo_name}' created successfully.\")\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "\n",
    "def push_to_hub(repo_name, token, step, max_retries=3, retry_delay=10):\n",
    "    api = HfApi(token=token)\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            # Upload the training state file.\n",
    "            api.upload_file(\n",
    "                path_or_fileobj=\"training_state.pt\",\n",
    "                path_in_repo=\"training_state.pt\",\n",
    "                repo_id=repo_name,\n",
    "                commit_message=f\"Training state at step {step}\"\n",
    "            )\n",
    "            # Here you can add code to push the model.\n",
    "            print(\"Training state (and model if implemented) pushed successfully.\")\n",
    "            break  # Exit the loop if the upload succeeds.\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt} failed: {e}\")\n",
    "            if attempt == max_retries:\n",
    "                print(\"Max attempts reached. Exiting.\")\n",
    "                raise e\n",
    "            time.sleep(retry_delay)\n",
    "\n",
    "# def push_to_hub_with_repo(repo_name, token, local_repo_dir, step, file_name=\"training_state.pt\", max_retries=3, retry_delay=10):\n",
    "\n",
    "#     # Create (or clone) the local copy of the repository if not present\n",
    "#     if not os.path.exists(local_repo_dir):\n",
    "#         os.makedirs(local_repo_dir, exist_ok=True)\n",
    "#         repo = Repository(local_dir=local_repo_dir, clone_from=repo_name, use_auth_token=token)\n",
    "#     else:\n",
    "#         repo = Repository(local_dir=local_repo_dir, clone_from=repo_name, use_auth_token=token)\n",
    "    \n",
    "#     # Copy the checkpoint file into the repository folder\n",
    "#     dst_file = os.path.join(local_repo_dir, file_name)\n",
    "#     import shutil\n",
    "#     shutil.copy(file_name, dst_file)\n",
    "    \n",
    "#     commit_message = f\"Training state at step {step}\"\n",
    "    \n",
    "#     # Retry logic\n",
    "#     for attempt in range(1, max_retries + 1):\n",
    "#         try:\n",
    "#             repo.git_add(auto_lfs_track=True)\n",
    "#             repo.git_commit(commit_message=commit_message)\n",
    "#             repo.git_push()\n",
    "#             print(\"Hub push completed using Repository.\")\n",
    "#             break  # Successful push; break out of loop.\n",
    "#         except Exception as e:\n",
    "#             print(f\"Attempt {attempt} failed during push: {e}\")\n",
    "#             if attempt == max_retries:\n",
    "#                 print(\"Max attempts reached. Exiting.\")\n",
    "#                 raise e\n",
    "#             time.sleep(retry_delay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective size with grad accumulation: 8\n",
      "Tokens per batch (paper has roughly .5M): 8192.0\n",
      "Total Training steps 443426.0\n",
      "N warmup steps (could be 3.00% of 443426.0 training_steps) => 13302 steps\n"
     ]
    }
   ],
   "source": [
    "class GPT2Config:\n",
    "    device: str = 'cpu'\n",
    "    from_checkpoint: bool = False\n",
    "    data_loader_batch_size = 4\n",
    "    warm_up_ratio: float = 0.03\n",
    "\n",
    "    n_files: int = 221713\n",
    "    rows_per_file = 16\n",
    "    tokens_per_row = 1024\n",
    "    n_tokens_per_file: int = rows_per_file*tokens_per_row # (file_batch_size x max_token_len)\n",
    "    total_tokens: int = n_files * n_tokens_per_file\n",
    "    gradient_accumulation_steps: int = 2\n",
    "    tokens_per_batch: int = (n_tokens_per_file/data_loader_batch_size) * gradient_accumulation_steps\n",
    "    print(f\"Effective size with grad accumulation: {data_loader_batch_size*gradient_accumulation_steps}\")\n",
    "    print(f\"Tokens per batch (paper has roughly .5M): {tokens_per_batch}\")\n",
    "\n",
    "    base_lr: float = 1e-4 # LR for should be 6e-4 to 2.5e-4 for gpt3 small-large\n",
    "    n_training_steps: float = total_tokens / tokens_per_batch\n",
    "    n_warmup_steps: int = int(round(n_training_steps * warm_up_ratio, 1))\n",
    "    print(f\"Total Training steps {n_training_steps}\")\n",
    "    print(f\"N warmup steps (could be {warm_up_ratio*100:.2f}% of {n_training_steps} training_steps) => {n_warmup_steps} steps\")\n",
    "\n",
    "    # beta1, beta2 = 0.9, 0.95 # these may need to be changed to fit our training assumptions\n",
    "    max_grad_norm = 1.0 # paper uses 1\n",
    "    weight_decay = .10 # i believe this still makes sense\n",
    "    num_epochs: int = 0\n",
    "\n",
    "    checkpoint_repo: str = None\n",
    "    save_file_name: str = \"training_state.pt\"\n",
    "    hf_token: str = None\n",
    "    start_file: str = None\n",
    "    save_steps = 100\n",
    "\n",
    "config = GPT2Config()\n",
    "config.from_checkpoint = True\n",
    "config.checkpoint_repo = \"cwestnedge/gpt2-test\"\n",
    "config.base_model = \"openai-community/gpt2\"\n",
    "config.hf_token = getpass.getpass(\"Enter your Hugging Face token: \")\n",
    "config.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# for testing\n",
    "config.save_steps=10\n",
    "config.base_lr=3e-4\n",
    "config.n_warmup_steps=1\n",
    "config.n_training_steps=40\n",
    "config.gradient_accumulation_steps=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "EntryNotFoundError",
     "evalue": "404 Client Error. (Request ID: Root=1-67f9a22f-516e9bbb100113f226410fff;576c7124-88c1-47f1-b57d-c73b5112965e)\n\nEntry Not Found for url: https://huggingface.co/cwestnedge/gpt2-test/resolve/main/training_state.pt.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/programming/git_hub/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:409\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/programming/git_hub/.venv/lib/python3.12/site-packages/requests/models.py:1024\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 404 Client Error: Not Found for url: https://huggingface.co/cwestnedge/gpt2-test/resolve/main/training_state.pt",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mEntryNotFoundError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# -------- load from checkpoint or start fresh --------\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.from_checkpoint: \n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     checkpoint = \u001b[43mload_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheckpoint_repo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhf_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_file_name\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     model.load_state_dict(checkpoint[\u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m]) \u001b[38;5;66;03m# we want to log model state dict eventually model.load_state_dict(model.state_dict())\u001b[39;00m\n\u001b[32m     22\u001b[39m     optimizer.load_state_dict(checkpoint[\u001b[33m'\u001b[39m\u001b[33moptimizer\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mload_checkpoint\u001b[39m\u001b[34m(repo_name, token, device, file_name)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_checkpoint\u001b[39m(repo_name, token, device, file_name=\u001b[33m\"\u001b[39m\u001b[33mtraining_state.pt\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     60\u001b[39m     repo_name = repo_name\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     training_state_path = \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m     checkpoint = torch.load(training_state_path, map_location=torch.device(device))\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m checkpoint\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/programming/git_hub/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/programming/git_hub/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:961\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m    941\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[32m    942\u001b[39m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[32m    943\u001b[39m         local_dir=local_dir,\n\u001b[32m   (...)\u001b[39m\u001b[32m    958\u001b[39m         local_files_only=local_files_only,\n\u001b[32m    959\u001b[39m     )\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m961\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m    963\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m    976\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/programming/git_hub/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1024\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1020\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m pointer_path\n\u001b[32m   1022\u001b[39m \u001b[38;5;66;03m# Try to get metadata (etag, commit_hash, url, size) from the server.\u001b[39;00m\n\u001b[32m   1023\u001b[39m \u001b[38;5;66;03m# If we can't, a HEAD request error is returned.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m (url_to_download, etag, commit_hash, expected_size, xet_file_data, head_call_error) = \u001b[43m_get_metadata_or_catch_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1028\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1029\u001b[39m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1030\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1031\u001b[39m \u001b[43m    \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1033\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1034\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1036\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrelative_filename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelative_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1037\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1039\u001b[39m \u001b[38;5;66;03m# etag can be None for several reasons:\u001b[39;00m\n\u001b[32m   1040\u001b[39m \u001b[38;5;66;03m# 1. we passed local_files_only.\u001b[39;00m\n\u001b[32m   1041\u001b[39m \u001b[38;5;66;03m# 2. we don't have a connection\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1047\u001b[39m \u001b[38;5;66;03m# If the specified revision is a commit hash, look inside \"snapshots\".\u001b[39;00m\n\u001b[32m   1048\u001b[39m \u001b[38;5;66;03m# If the specified revision is a branch or tag, look inside \"refs\".\u001b[39;00m\n\u001b[32m   1049\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m head_call_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1050\u001b[39m     \u001b[38;5;66;03m# Couldn't make a HEAD call => let's try to find a local file\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/programming/git_hub/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1484\u001b[39m, in \u001b[36m_get_metadata_or_catch_error\u001b[39m\u001b[34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[39m\n\u001b[32m   1482\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1483\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1484\u001b[39m         metadata = \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1485\u001b[39m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\n\u001b[32m   1486\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1487\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[32m   1488\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m storage_folder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m relative_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1489\u001b[39m             \u001b[38;5;66;03m# Cache the non-existence of the file\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/programming/git_hub/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/programming/git_hub/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1401\u001b[39m, in \u001b[36mget_hf_file_metadata\u001b[39m\u001b[34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[39m\n\u001b[32m   1398\u001b[39m hf_headers[\u001b[33m\"\u001b[39m\u001b[33mAccept-Encoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33midentity\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# prevent any compression => we want to know the real size of the file\u001b[39;00m\n\u001b[32m   1400\u001b[39m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1401\u001b[39m r = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1402\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHEAD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1404\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1406\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1408\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1409\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1410\u001b[39m hf_raise_for_status(r)\n\u001b[32m   1412\u001b[39m \u001b[38;5;66;03m# Return\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/programming/git_hub/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:285\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    283\u001b[39m \u001b[38;5;66;03m# Recursively follow relative redirects\u001b[39;00m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     response = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[32m    293\u001b[39m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[32m    294\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m300\u001b[39m <= response.status_code <= \u001b[32m399\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/programming/git_hub/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:309\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[32m    308\u001b[39m response = get_session().request(method=method, url=url, **params)\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/programming/git_hub/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:420\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    418\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m error_code == \u001b[33m\"\u001b[39m\u001b[33mEntryNotFound\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    419\u001b[39m     message = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Client Error.\u001b[39m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEntry Not Found for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(EntryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m error_code == \u001b[33m\"\u001b[39m\u001b[33mGatedRepo\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    423\u001b[39m     message = (\n\u001b[32m    424\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Client Error.\u001b[39m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot access gated repo for url \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    425\u001b[39m     )\n",
      "\u001b[31mEntryNotFoundError\u001b[39m: 404 Client Error. (Request ID: Root=1-67f9a22f-516e9bbb100113f226410fff;576c7124-88c1-47f1-b57d-c73b5112965e)\n\nEntry Not Found for url: https://huggingface.co/cwestnedge/gpt2-test/resolve/main/training_state.pt."
     ]
    }
   ],
   "source": [
    "# -------- Initialize mode, optimizer and lr_scheduler -------- \n",
    "model = load_base_model(model_name=config.base_model, device=config.device)\n",
    "model_grouped_params = get_grouped_params(model, weight_decay=config.weight_decay)\n",
    "optimizer = initialize_optimizer(model_grouped_params, base_lr=config.base_lr)\n",
    "lr_scheduler = initialize_scheduler(\n",
    "    n_warmup_steps=config.n_warmup_steps, \n",
    "    n_training_steps=config.n_training_steps, \n",
    "    optimizer=optimizer\n",
    ")\n",
    "scaler = initialize_scaler(config.device)\n",
    "\n",
    "# -------- load from checkpoint or start fresh --------\n",
    "if config.from_checkpoint: \n",
    "    checkpoint = load_checkpoint(\n",
    "        repo_name=config.checkpoint_repo,\n",
    "        token=config.hf_token,\n",
    "        device=config.device, \n",
    "        file_name=config.save_file_name\n",
    "    )\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model']) # we want to log model state dict eventually model.load_state_dict(model.state_dict())\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "    if scaler:\n",
    "        scaler.load_state_dict(checkpoint['scaler'])\n",
    "    \n",
    "    global_step = checkpoint['global_step']\n",
    "    loss_history = checkpoint['losses']\n",
    "    last_file = checkpoint['batch_file']\n",
    "    last_file = ''.join(last_file)\n",
    "\n",
    "    train_files_full = sorted(glob.glob(\"../processed_batches/train/*.pt\"), key=extract_file_numbers)\n",
    "    start_file_path = f'../processed_batches/train/{last_file}'\n",
    "    start_idx = train_files_full.index(start_file_path)\n",
    "    train_files_ = train_files_full[start_idx:] # fix this after testing to train_files_full[start_idx+1:]\n",
    "    print()\n",
    "    print(f'Last processed file {last_file}. Resuming run from {train_files_[0]}')\n",
    "    print(f\"{(len(train_files_)/len(train_files_full))*100:0.3f}% remaining...\")\n",
    "\n",
    "else:\n",
    "    global_step, loss_history= 0, []\n",
    "    train_files_ = sorted(glob.glob(\"../processed_batches/train/*.pt\"), key=extract_file_numbers)\n",
    "    print()\n",
    "    print(f'training run from {train_files_[0]}')\n",
    "\n",
    "\n",
    "\n",
    "train_ds = PTIterableDataset(train_files_)\n",
    "train_loader = DataLoader(train_ds, batch_size=2, num_workers=0, drop_last=True)\n",
    "print(next(iter(train_loader)))\n",
    "print()\n",
    "\n",
    "create_repo_if_not_exists(config.checkpoint_repo, config.hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU Training Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global step 21, LR: 0.00030000, Loss: 3.3392\n",
      "Global step 22, LR: 0.00029951, Loss: 3.3392\n",
      "Global step 23, LR: 0.00029806, Loss: 3.1993\n",
      "Global step 24, LR: 0.00029564, Loss: 3.0980\n",
      "Global step 25, LR: 0.00029228, Loss: 3.0116\n",
      "Global step 26, LR: 0.00028800, Loss: 2.9324\n",
      "Global step 27, LR: 0.00028282, Loss: 2.8589\n",
      "Global step 28, LR: 0.00027678, Loss: 2.7905\n",
      "Global step 29, LR: 0.00026992, Loss: 2.7258\n",
      "Global step 30, LR: 0.00026228, Loss: 2.6642\n",
      "saved checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/collinswestnedge/Desktop/programming/git_hub/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\n",
      "For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Looks like you do not have git-lfs installed, please install. You can install from https://git-lfs.github.com/. Then run `git lfs install` (you only have to do this once).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/programming/git_hub/.venv/lib/python3.12/site-packages/huggingface_hub/repository.py:592\u001b[39m, in \u001b[36mRepository.check_git_versions\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m592\u001b[39m     lfs_version = \u001b[43mrun_subprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgit-lfs --version\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[43m)\u001b[49m.stdout.strip()\n\u001b[32m    593\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/programming/git_hub/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_subprocess.py:85\u001b[39m, in \u001b[36mrun_subprocess\u001b[39m\u001b[34m(command, folder, check, **kwargs)\u001b[39m\n\u001b[32m     83\u001b[39m     folder = \u001b[38;5;28mstr\u001b[39m(folder)\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstderr\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstdout\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreplace\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# if not utf-8, replace char by �\u001b[39;49;00m\n\u001b[32m     92\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetcwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.7_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/subprocess.py:548\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    546\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33mstderr\u001b[39m\u001b[33m'\u001b[39m] = PIPE\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[32m    549\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.7_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/subprocess.py:1026\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[39m\n\u001b[32m   1023\u001b[39m             \u001b[38;5;28mself\u001b[39m.stderr = io.TextIOWrapper(\u001b[38;5;28mself\u001b[39m.stderr,\n\u001b[32m   1024\u001b[39m                     encoding=encoding, errors=errors)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1028\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1029\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1030\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1031\u001b[39m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1033\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1034\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m   1036\u001b[39m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.7_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/subprocess.py:1955\u001b[39m, in \u001b[36mPopen._execute_child\u001b[39m\u001b[34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session, process_group)\u001b[39m\n\u001b[32m   1954\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m err_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1955\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[32m   1956\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'git-lfs'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     38\u001b[39m                 \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33msaved checkpoint\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     39\u001b[39m                 \u001b[38;5;66;03m# push_to_hub(\u001b[39;00m\n\u001b[32m     40\u001b[39m                 \u001b[38;5;66;03m#     repo_name=config.checkpoint_repo,\u001b[39;00m\n\u001b[32m     41\u001b[39m                 \u001b[38;5;66;03m#     token=config.hf_token,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     44\u001b[39m                 \u001b[38;5;66;03m#     retry_delay=10\u001b[39;00m\n\u001b[32m     45\u001b[39m                 \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m                 \u001b[43mpush_to_hub_with_repo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mrepo_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheckpoint_repo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhf_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mlocal_repo_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_repo_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mglobal_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_file_name\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m                 \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mhub push completed\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# print('final model push...')\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# model.push_to_hub(config.checkpoint_repo, commit_message=f\"trained model at pass {epoch}\")\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 167\u001b[39m, in \u001b[36mpush_to_hub_with_repo\u001b[39m\u001b[34m(repo_name, token, local_repo_dir, step, file_name, max_retries, retry_delay)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(local_repo_dir):\n\u001b[32m    166\u001b[39m     os.makedirs(local_repo_dir, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     repo = \u001b[43mRepository\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_repo_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclone_from\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    169\u001b[39m     repo = Repository(local_dir=local_repo_dir, clone_from=repo_name, use_auth_token=token)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/programming/git_hub/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/programming/git_hub/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:132\u001b[39m, in \u001b[36m_deprecate_method.<locals>._inner_deprecate_method.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    130\u001b[39m     warning_message += \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m + message\n\u001b[32m    131\u001b[39m warnings.warn(warning_message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/programming/git_hub/.venv/lib/python3.12/site-packages/huggingface_hub/repository.py:522\u001b[39m, in \u001b[36mRepository.__init__\u001b[39m\u001b[34m(self, local_dir, clone_from, repo_type, token, git_user, git_email, revision, skip_lfs_files, client)\u001b[39m\n\u001b[32m    519\u001b[39m \u001b[38;5;28mself\u001b[39m.skip_lfs_files = skip_lfs_files\n\u001b[32m    520\u001b[39m \u001b[38;5;28mself\u001b[39m.client = client \u001b[38;5;28;01mif\u001b[39;00m client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m HfApi()\n\u001b[32m--> \u001b[39m\u001b[32m522\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcheck_git_versions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    525\u001b[39m     \u001b[38;5;28mself\u001b[39m.huggingface_token: Optional[\u001b[38;5;28mstr\u001b[39m] = token\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/programming/git_hub/.venv/lib/python3.12/site-packages/huggingface_hub/repository.py:594\u001b[39m, in \u001b[36mRepository.check_git_versions\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    592\u001b[39m     lfs_version = run_subprocess(\u001b[33m\"\u001b[39m\u001b[33mgit-lfs --version\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m.local_dir).stdout.strip()\n\u001b[32m    593\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m594\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[32m    595\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mLooks like you do not have git-lfs installed, please install.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    596\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m You can install from https://git-lfs.github.com/.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    597\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m Then run `git lfs install` (you only have to do this once).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    598\u001b[39m     )\n\u001b[32m    599\u001b[39m logger.info(git_version + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + lfs_version)\n",
      "\u001b[31mOSError\u001b[39m: Looks like you do not have git-lfs installed, please install. You can install from https://git-lfs.github.com/. Then run `git lfs install` (you only have to do this once)."
     ]
    }
   ],
   "source": [
    "# for epoch in range(config.n_training_steps):\n",
    "model.train();\n",
    "running_loss = 0 \n",
    "for step, batch in enumerate(train_loader, start=1): \n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask = batch['attention_mask']\n",
    "    current_file = set(batch['files'])\n",
    "\n",
    "    # forward pass (no autocast for CPU)\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "    raw_loss = outputs.loss\n",
    "\n",
    "    running_loss+=raw_loss.item()\n",
    "    loss = raw_loss/config.gradient_accumulation_steps\n",
    "    loss.backward()\n",
    "\n",
    "    if step % config.gradient_accumulation_steps == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        lr_scheduler.step()\n",
    "        global_step+=1\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        effective_loss = running_loss/config.gradient_accumulation_steps\n",
    "        loss_history.append(effective_loss)\n",
    "        print(f\"Global step {global_step}, LR: {current_lr:.8f}, Loss: {effective_loss:.4f}\")\n",
    "        running_loss = 0 \n",
    "\n",
    "        if global_step % config.save_steps == 0:\n",
    "            save_checkpoint(\n",
    "                model=model, \n",
    "                optimizer=optimizer, \n",
    "                lr_scheduler=lr_scheduler, \n",
    "                global_step=global_step, \n",
    "                loss_history=loss_history,\n",
    "                last_file=current_file,\n",
    "            )\n",
    "            \n",
    "            print('saved checkpoint')\n",
    "            push_to_hub(\n",
    "                repo_name=config.checkpoint_repo,\n",
    "                token=config.hf_token,\n",
    "                step=global_step,\n",
    "                max_retries=3,\n",
    "                retry_delay=10\n",
    "            )\n",
    "            print('hub push completed')\n",
    "\n",
    "# print('final model push...')\n",
    "# model.push_to_hub(config.checkpoint_repo, commit_message=f\"trained model at pass {epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Training Loop (FP16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train();\n",
    "running_loss = 0\n",
    "for step, batch in enumerate(train_loader, start=1):\n",
    "    input_ids = batch['input_ids'].to(config.device)\n",
    "    attention_mask = batch['attention_mask'].to(config.device)\n",
    "    current_file = batch['files'][0]  # single filename for consistency\n",
    "\n",
    "    with torch.autocast(device_type=\"cuda\"):\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "        raw_loss = outputs.loss\n",
    "\n",
    "    running_loss += raw_loss.item()\n",
    "    loss = raw_loss / config.gradient_accumulation_steps\n",
    "    scaler.scale(loss).backward()\n",
    "\n",
    "    if step % config.gradient_accumulation_steps == 0:\n",
    "        # unscale, clip, step, update scaler & scheduler\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        lr_scheduler.step()\n",
    "        global_step += 1\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        effective_loss = running_loss / config.gradient_accumulation_steps\n",
    "        loss_history.append(effective_loss)\n",
    "        print(f\"Global step {global_step}, LR: {current_lr:.8f}, Loss: {effective_loss:.4f}\")\n",
    "        running_loss = 0\n",
    "\n",
    "        if global_step % config.save_steps == 0:\n",
    "            save_checkpoint(\n",
    "                model=model,\n",
    "                optimizer=optimizer,\n",
    "                lr_scheduler=lr_scheduler,\n",
    "                global_step=global_step,\n",
    "                loss_history=loss_history,\n",
    "                last_file=current_file,\n",
    "                scaler=scaler\n",
    "            )\n",
    "            print('saved checkpoint')\n",
    "            push_to_hub(\n",
    "                repo_name=config.checkpoint_repo,\n",
    "                token=config.hf_token,\n",
    "                step=global_step\n",
    "            )\n",
    "            print('hub push completed')\n",
    "\n",
    "# print('final model push...')\n",
    "# model.push_to_hub(config.checkpoint_repo, commit_message=\"Final trained model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
