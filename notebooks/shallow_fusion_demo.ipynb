{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torch transformers datasets tqdm lxml librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import torch.nn.functional as F\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, GPT2LMHeadModel, AutoTokenizer\n",
    "import math\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load ASR model and audio\n",
    "- the correct text we want to predict is \"The patient exhibits signs of bradykinesia, a common symptom in Parkinson's\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tw/9mx1zhwx2hd603sslv56_2lc0000gn/T/ipykernel_20071/2154953300.py:3: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio_array, sampling_rate = librosa.load(audio_file, sr=16000)\n",
      "/Users/collinswestnedge/Desktop/programming/git_hub/.venv/lib/python3.12/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    }
   ],
   "source": [
    "# Load your audio file and resample it to 16 kHz (Whisper's expected rate)\n",
    "audio_file = \"../assets_folder/my_procedure.m4a\"\n",
    "audio_array, sampling_rate = librosa.load(audio_file, sr=16000)\n",
    "\n",
    "# Initialize the processor and model\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n",
    "decoder = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")\n",
    "\n",
    "# Prepare the input features from the audio array\n",
    "input_features = processor(audio_array, sampling_rate=sampling_rate, return_tensors=\"pt\").input_features\n",
    "\n",
    "# Begin the decoding process with the decoder start token\n",
    "decoder_input_ids = torch.tensor([[decoder.config.decoder_start_token_id]])\n",
    "decoder.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in LM expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"cwestnedge/gpt2-medium-pubmed\")\n",
    "gpt2_model.eval(); # put GPT2 in eval mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run shallow fusion\n",
    "Token prediction is implemented in stages. Early on we rely solely on the ASR model and then gradually introduce the language model as more context becomes available. For example:\n",
    "\n",
    "$$\n",
    "\\textbf{Token Selection at Step }t:\\quad\n",
    "F(x,t)\n",
    "=\n",
    "\\begin{cases}\n",
    "\\displaystyle\n",
    "\\arg\\max_{y_t}\\;\\log P_{\\text{ASR}}(y_t \\!\\mid\\! x,\\; y_{<t}),\n",
    "& t < \\text{initial\\_steps},\\\\[0.75em]\n",
    "\\displaystyle\n",
    "\\arg\\max_{y_t}\\;\\Bigl[\n",
    "\\log P_{\\text{ASR}}(y_t \\!\\mid\\! x,\\; y_{<t})\n",
    "\\;+\\;\n",
    "\\lambda\\,\\log P_{\\text{LM}}(y_t \\!\\mid\\! y_{<t})\n",
    "\\Bigr],\n",
    "& t \\ge \\text{initial\\_steps}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This piecewise approach allows the system to build confidence from the raw audio transcription initially before incorporating the domain expert corrections, namely because our starting point should be conditionalized on something observed e.g. we gotta start somewhere. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    }
   ],
   "source": [
    "# max_new_tokens = 100   # Maximum tokens to generate (you can adjust this)\n",
    "initial_steps = 2     # Use pure Whisper prediction for the first few tokens\n",
    "num_steps = 100        # For demonstration, we run 6 iterations (could be max_new_tokens)\n",
    "alpha = 0.25\n",
    "\n",
    "# Start with Whisper's decoder start token.\n",
    "# Load whisper's special tokens so we can filter out for GPT2.\n",
    "\n",
    "decoder_input_ids = torch.tensor([[decoder.config.decoder_start_token_id]], device=input_features.device)\n",
    "whisper_special_ids = processor.tokenizer.all_special_ids\n",
    "\n",
    "# Initialize a GPT2 input sequence (will be updated after each step).\n",
    "# Initially, filter the decoder_input_ids to remove Whisper's special tokens.\n",
    "\n",
    "gpt_input_ids = [token for token in decoder_input_ids[0].tolist() if token not in whisper_special_ids]\n",
    "if not gpt_input_ids:\n",
    "    gpt_input_ids = decoder_input_ids[0].tolist()\n",
    "gpt_input_tensor = torch.tensor([gpt_input_ids], device=decoder_input_ids.device)\n",
    "\n",
    "\n",
    "whisper_data = []\n",
    "gpt2_data = []\n",
    "step_tokens = []\n",
    "with torch.no_grad():\n",
    "    for step in range(num_steps):\n",
    "        decoder_outputs = decoder(input_features, decoder_input_ids=decoder_input_ids, use_cache=True)\n",
    "        decoder_logits = decoder_outputs.logits[:, -1, :]  # shape: [batch, whisper_vocab_size]\n",
    "\n",
    "        if step < initial_steps:\n",
    "            # for the first few steps we need to use whisper's decoder prediction b/c we gotta start somewhere\n",
    "            # the reason initial steps is < 2 is because the first two tokens are for whisper are special tokens\n",
    "            next_token = decoder_logits.argmax(dim=-1, keepdim=True)\n",
    "        else:\n",
    "            # if we're past the initial generation steps from decoder we can now leverage gpt2\n",
    "            # which excludes those special tokens froms whisper\n",
    "            gpt2_outputs = gpt2_model(gpt_input_tensor)\n",
    "            gpt2_logits = gpt2_outputs.logits[:, -1, :]  # shape: [batch, gpt2_vocab_size]\n",
    "\n",
    "            # compute log-probabilities for both models\n",
    "            whisper_log_probs = F.log_softmax(decoder_logits, dim=-1)\n",
    "            gpt2_log_probs = F.log_softmax(gpt2_logits, dim=-1)\n",
    "\n",
    "            # create index for shared domain assuming index 0 to len(gpt2 vocab size) is a subset of whisper's.\n",
    "            shared_domain_idx = gpt2_log_probs.shape[-1]  # e.g. 50257\n",
    "\n",
    "            # restrict whisper's log-probs to the GPT2 vocab\n",
    "            fused_logits = whisper_log_probs[:, :shared_domain_idx] + alpha * gpt2_log_probs\n",
    "\n",
    "            # this is how to do it if we dont care about the difference in vocabularies\n",
    "            next_token = fused_logits.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "            # this is so we dont miss anything we excluded in the fusion for final prediction\n",
    "            # it will need to be calibrated so its on the same scale as the fused logits though\n",
    "            # whisper_only_logits = whisper_log_probs[:,shared_domain_idx:]\n",
    "            # fused_and_extended_logits = torch.cat([fused_logits, whisper_only_logits], dim=-1)\n",
    "            # next_token = fused_and_extended_logits.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "        # append the chosen token to Whisper's full decoder input.\n",
    "        decoder_input_ids = torch.cat([decoder_input_ids, next_token], dim=-1)\n",
    "\n",
    "        # update the GPT2 input sequence by filtering out Whisper special tokens.\n",
    "        new_decoder_ids = decoder_input_ids[0].tolist()\n",
    "        gpt_input_ids = [token for token in new_decoder_ids if token not in whisper_special_ids]\n",
    "        if not gpt_input_ids:\n",
    "            gpt_input_ids = new_decoder_ids  # fallback in the unlikely event all tokens are special\n",
    "        gpt_input_tensor = torch.tensor([gpt_input_ids], device=decoder_input_ids.device)\n",
    "\n",
    "        # stop if the end-of-sequence token is generated.\n",
    "        if next_token.item() == processor.tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "# Finally, decode the full Whisper sequence (special tokens will be handled as needed).\n",
    "final_output = processor.batch_decode(decoder_input_ids, skip_special_tokens=True)\n",
    "# print(\"Final output:\", final_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predicted output with whisper/ASR model only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of box whisper prediction:\n",
      "Hi, my name is Collins and I'm calling about a procedure that I had related to tetrology if below. I wanted to check on the status and make sure that the claim is being processed. It was for trans-cathodar pulmonary valve replacement on October at Northwestern Medicine.\n"
     ]
    }
   ],
   "source": [
    "whisper_only_predicted_ids = decoder.generate(input_features)\n",
    "whisper_only_transcription = processor.batch_decode(whisper_only_predicted_ids, skip_special_tokens=True)\n",
    "print(\"Out of box whisper prediction:\")\n",
    "print(whisper_only_transcription[0].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predicted output when leveraging shallow fusion + LM expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction with shallow fusion:\n",
      "Hi, my name is Collins and I'm calling about a procedure that I had related to tetralogy of Fallot. I wanted to check on the status and make sure that the claim is being processed. It was for transcatheter pulmonary valve replacement on October at Northwestern Medicine.\n"
     ]
    }
   ],
   "source": [
    "# Finally, decode the full Whisper sequence (special tokens will be handled as needed).\n",
    "final_output = processor.batch_decode(decoder_input_ids, skip_special_tokens=True)\n",
    "print(\"Prediction with shallow fusion:\")\n",
    "print(final_output[0].strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
